"""
Enhanced Gemini Helper with Context Awareness
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json

class GeminiQMDGHelper:
    """Helper class with context awareness for QMDG analysis"""
    
    def __init__(self, api_key):
        """Initialize Gemini with API key"""
        self.api_key = api_key
        genai.configure(api_key=api_key)
        
        # Context tracking - Initialize BEFORE model selection
        self.current_context = {
            'topic': None,
            'palace': None,
            'chart_data': None,
            'last_action': None,
            'dung_than': []
        }
        
        # Adaptive model selection
        self.model = self._get_best_model()

        # n8n endpoint (optional)
        self.n8n_url = None
    
    def set_n8n_url(self, url):
        """Set n8n webhook URL for processing"""
        self.n8n_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Prioritize 1.5 Pro because "gemini t·ªët nh·∫•t"
        models_to_try = [
            'gemini-2.0-flash-exp', # Try latest 2.0 flash
            'gemini-1.5-pro-latest', 
            'gemini-1.5-pro',
            'gemini-1.5-flash-latest', 
            'gemini-1.5-flash',
            'gemini-pro', 
            'gemini-1.0-pro'
        ]
        
        last_error = "Unknown error"
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Quick test with low tokens
                model.generate_content("ping", generation_config={"max_output_tokens": 1})
                return model
            except Exception as e:
                last_error = str(e)
                continue
        
        # Fallback to list models if configured ones fail
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    name = m.name.split('/')[-1]
                    try:
                        model = genai.GenerativeModel(name)
                        model.generate_content("ping", generation_config={"max_output_tokens": 1})
                        return model
                    except: continue
        except Exception: pass
        
        # Ultimate fallback but store error info
        self.last_startup_error = last_error
        return genai.GenerativeModel('gemini-1.5-flash') # Default to flash as it's more widely available

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?", generation_config={"max_output_tokens": 20})
            if response.text:
                return True, "K·∫øt n·ªëi th√†nh c√¥ng!"
            return False, "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
        except Exception as e:
            error_msg = str(e)
            if "API_KEY_INVALID" in error_msg:
                return False, "API Key kh√¥ng ch√≠nh x√°c ho·∫∑c ƒë√£ h·∫øt h·∫°n."
            elif "quota" in error_msg.lower():
                return False, "ƒê√£ h·∫øt h·∫°n m·ª©c s·ª≠ d·ª•ng (Quota) cho Key n√†y."
            return False, f"L·ªói k·∫øt n·ªëi: {error_msg}"

    def _call_ai(self, prompt):
        """Call AI via n8n or direct Gemini API"""
        # Option 1: Use n8n if configured
        if self.n8n_url:
            try:
                payload = {
                    "prompt": prompt,
                    "api_key": self.api_key
                }
                headers = {"Content-Type": "application/json"}
                response = requests.post(self.n8n_url, json=payload, headers=headers, timeout=60)
                if response.status_code == 200:
                    text = response.json().get('text', '')
                    if text: return text
                    # If empty text, fallback might be needed or return empty
                else:
                    print(f"n8n Error: {response.text}")
            except Exception as e:
                print(f"n8n Exception: {e}")
                # Fallback to local
        
        # Option 2: Direct Gemini API
        try:
            response = self.model.generate_content(prompt)
            if not response.text:
                return "‚ö†Ô∏è AI tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ªëng. Th·ª≠ l·∫°i sau ho·∫∑c ki·ªÉm tra API Key."
            return response.text
        except Exception as e:
            error_msg = str(e)
            if "finish_reason: SAFETY" in error_msg:
                return "üõ°Ô∏è N·ªôi dung b·ªã AI ch·∫∑n do vi ph·∫°m quy t·∫Øc an to√†n. Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c."
            raise e # Let the helper handle more complex errors if needed
    
    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        knowledge = """
**QUY T·∫ÆC LU·∫¨N GI·∫¢I CHUY√äN S√ÇU:**
1. **Nguy√™n l√Ω Sinh Kh·∫Øc Cung:** 
   - Th·ªßy (1) -> M·ªôc (3,4) -> H·ªèa (9) -> Th·ªï (2,8,5) -> Kim (6,7) -> Th·ªßy (1).
   - Kh·∫Øc: Th·ªßy kh·∫Øc H·ªèa, H·ªèa kh·∫Øc Kim, Kim kh·∫Øc M·ªôc, M·ªôc kh·∫Øc Th·ªï, Th·ªï kh·∫Øc Th·ªßy.
2. **D·ª•ng Th·∫ßn (Object):** L√† y·∫øu t·ªë ƒë·∫°i di·ªán cho s·ª± vi·ªác c·∫ßn xem.
3. **B·∫£n Th√¢n (Subject):** ƒê·∫°i di·ªán b·ªüi Can Ng√†y (Thi√™n b√†n) ho·∫∑c cung c·ªßa ng∆∞·ªùi h·ªèi.
4. **Ph√¢n t√≠ch n·ªôi cung:** 
   - Sao (Thi√™n th·ªùi), M√¥n (ƒê·ªãa l·ª£i - Nh√¢n h√≤a), Th·∫ßn (Th·∫ßn tr·ª£), Kh√¥ng Vong (Tr·∫°ng th√°i r·ªóng, ch∆∞a t·ªõi l√∫c ho·∫∑c th·∫•t b·∫°i).
5. **K·∫æT LU·∫¨N:** D·ª±a tr√™n vi·ªác Cung D·ª•ng Th·∫ßn Sinh cho hay Kh·∫Øc Cung B·∫£n Th·∫ßn (ho·∫∑c ng∆∞·ª£c l·∫°i).
"""
        return knowledge

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        
        # Add system-wide knowledge
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if self.current_context.get('palace'):
            palace = self.current_context['palace']
            context_parts.append(f"**ƒêang xem cung:** Cung {palace.get('num', 'N/A')} - {palace.get('qua', 'N/A')}")
            context_parts.append(f"  - Sao: {palace.get('star', 'N/A')}")
            context_parts.append(f"  - M√¥n: {palace.get('door', 'N/A')}")
            context_parts.append(f"  - Th·∫ßn: {palace.get('deity', 'N/A')}")
        
        if self.current_context.get('dung_than'):
            context_parts.append(f"**D·ª•ng Th·∫ßn:** {', '.join(self.current_context['dung_than'])}")
        
        if self.current_context.get('last_action'):
            context_parts.append(f"**H√†nh ƒë·ªông tr∆∞·ªõc:** {self.current_context['last_action']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH V√Ä KI·∫æN TH·ª®C HI·ªÜN T·∫†I:**"] + context_parts) + "\n\n"
        return ""
    
    def analyze_palace(self, palace_data, topic):
        """
        Analyze a specific palace with AI - FOCUS ON ESSENTIALS
        """
        # Update context
        self.update_context(
            topic=topic,
            palace=palace_data,
            last_action=f"Ph√¢n t√≠ch Cung {palace_data.get('num')}"
        )
        
        context = self.get_context_prompt()
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p h√†ng ƒë·∫ßu. H√£y ph√¢n t√≠ch Cung {palace_data.get('num', 'N/A')} cho ch·ªß ƒë·ªÅ: **{topic}**.

**NGUY√äN T·∫ÆC: NG·∫ÆN G·ªåN - ƒê·ª¶ √ù - KH√îNG LAN MAN.**

**Y√™u c·∫ßu:**
1. **Gi√° tr·ªã c·ªßa cung**: Cung n√†y l√† Thu·∫≠n hay Ngh·ªãch cho vi·ªác "{topic}"?
2. **ƒêi·ªÉm nh·∫•n ch√≠nh**: T·ªï h·ª£p Sao/M√¥n/Th·∫ßn/Can t·∫°i ƒë√¢y b√°o hi·ªáu ƒëi·ªÅu g√¨ c·ªët l√µi nh·∫•t?
3. **Chi·∫øn thu·∫≠t h√†nh ƒë·ªông**: L√†m g√¨ ngay t·∫°i cung n√†y ƒë·ªÉ ƒë·∫°t m·ª•c ti√™u?

Tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng ch√†o h·ªèi, kh√¥ng d·∫´n nh·∫≠p."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i."
    
    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None):
        """
        Focused Relational Analysis: Self (Subject) vs Topic (Object)
        """
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            dung_than=dung_than_info or [],
            last_action="Lu·∫≠n gi·∫£i tr·ªçng t√¢m D·ª•ng Th·∫ßn"
        )
        
        # 1. SCAN FOR CORE ACTORS
        can_ngay = chart_data.get('can_ngay', 'N/A')
        can_gio = chart_data.get('can_gio', 'N/A')
        truc_phu = chart_data.get('truc_phu_ten', 'N/A')
        truc_su = chart_data.get('truc_su_ten', 'N/A')
        
        self_palace = "?"
        dung_than_details = []
        process_palace = "?"
        
        for i in range(1, 10):
            # Locate Self
            if chart_data.get('can_thien_ban', {}).get(i) == can_ngay:
                self_palace = str(i)
            # Locate Process/Outcome
            if chart_data.get('can_thien_ban', {}).get(i) == can_gio:
                process_palace = str(i)
            # Locate D·ª•ng Th·∫ßn
            if dung_than_info:
                for dt in dung_than_info:
                    door_val = chart_data.get('nhan_ban', {}).get(i)
                    if (chart_data.get('thien_ban', {}).get(i) == dt or 
                        door_val == dt or 
                        chart_data.get('than_ban', {}).get(i) == dt or 
                        chart_data.get('can_thien_ban', {}).get(i) == dt or
                        (dt.endswith(" M√¥n") and door_val and door_val in dt)):
                        # Get details of D·ª•ng Th·∫ßn Palace
                        detail = {
                            'dt': dt,
                            'palace': i,
                            'star': chart_data.get('thien_ban', {}).get(i),
                            'door': door_val,
                            'deity': chart_data.get('than_ban', {}).get(i),
                            'void': i in chart_data.get('khong_vong', [])
                        }
                        dung_than_details.append(detail)
        
        # 2. CONTEXTUAL PROMPT
        prompt = f"""{self.get_context_prompt()}B·∫°n l√† b·∫≠c th·∫ßy K·ª≥ M√¥n ƒê·ªôn Gi√°p. H√£y th·ª±c hi·ªán lu·∫≠n gi·∫£i LU·∫¨N GI·∫¢I TR·ªåNG T√ÇM cho ch·ªß ƒë·ªÅ: **{topic}**.

**QUY T·∫ÆC C·ªêT L√ïI:**
- KH√îNG li·ªát k√™ t·∫•t c·∫£ 9 cung. 
- CH·ªà t·∫≠p trung v√†o m·ªëi quan h·ªá gi·ªØa **B·∫£n Th√¢n ({can_ngay})** v√† **D·ª•ng Th·∫ßn ({topic})**.
- K·∫æT LU·∫¨N d·ª©t kho√°t d·ª±a tr√™n Sinh/Kh·∫Øc/Ch·∫ø/H√≥a.

**TH√îNG TIN KEY TRONG B√ÄN:**
1. **B·∫£n Th√¢n (Ng∆∞·ªùi h·ªèi):** Cung {self_palace} (Sao {chart_data.get('thien_ban', {}).get(int(self_palace) if self_palace.isdigit() else 1)}, M√¥n {chart_data.get('nhan_ban', {}).get(int(self_palace) if self_palace.isdigit() else 1)}).
2. **D·ª•ng Th·∫ßn ({topic}):** {', '.join([f"{d['dt']} t·∫°i Cung {d['palace']} (Sao {d['star']}, M√¥n {d['door']}, Th·∫ßn {d['deity']}{', KH√îNG VONG' if d['void'] else ''})" for d in dung_than_details])}.
3. **Di·ªÖn bi·∫øn (Can Gi·ªù):** Cung {process_palace}.
4. **C∆° c·∫•u l√£nh ƒë·∫°o:** Tr·ª±c Ph√π l√† {truc_phu}, Tr·ª±c S·ª≠ l√† {truc_su}.

**Y√äU C·∫¶U: TR·∫¢ L·ªúI S·∫ÆC L·∫†NH, S√öC T√çCH, KH√îNG CH·ª¶ GI·∫¢I L√ù THUY·∫æT.**

- **PH·∫¶N 1: K·∫æT QU·∫¢ (3-4 d√≤ng):** Sinh hay Kh·∫Øc? C√°t hay Hung? Ng·∫Øn g·ªçn nh·∫•t c√≥ th·ªÉ.
- **PH·∫¶N 2: DI·ªÑN BI·∫æN (Tr·ª±c Ph√π/Tr·ª±c S·ª≠):** Xu th·∫ø v√† h√†nh ƒë·ªông c·ªët l√µi.
- **PH·∫¶N 3: CHI·∫æN THU·∫¨T (1 c√¢u duy nh·∫•t):** Ph·∫£i l√†m g√¨ ngay b√¢y gi·ªù.
- **PH·∫¶N 4: TH·ªúI ƒêI·ªÇM (1 c·ª•m t·ª´):** Khi n√†o.

H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c·ª±c k·ª≥ s√∫c t√≠ch, t·∫≠p trung 100% v√†o ch·ªß ƒë·ªÅ {topic}."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS
        """
        # Use stored context if not provided
        if chart_data is None:
            chart_data = self.current_context.get('chart_data')
        if topic is None:
            topic = self.current_context.get('topic', 'Chung')
        
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            last_action=f"H·ªèi: {question[:50]}..."
        )
        
        context = self.get_context_prompt()
        
        # Build chart context if available
        chart_context = ""
        if chart_data:
            palace_summary = []
            for i in range(1, 10):
                palace_summary.append(
                    f"Cung {i}: {chart_data.get('thien_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('nhan_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('than_ban', {}).get(i, 'N/A')}"
                )
            chart_context = "\n**B√†n K·ª≥ M√¥n hi·ªán t·∫°i:**\n" + "\n".join(palace_summary)
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

**B·ªëi c·∫£nh:**
- Ch·ªß ƒë·ªÅ: {topic}
{chart_context}

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n:
1. Ng·ªØ c·∫£nh hi·ªán t·∫°i (ch·ªß ƒë·ªÅ, cung ƒëang xem, h√†nh ƒë·ªông tr∆∞·ªõc)
2. Th√¥ng tin t·ª´ b√†n K·ª≥ M√¥n (n·∫øu c√≥)
3. Ki·∫øn th·ª©c v·ªÅ d·ªãch h·ªçc
4. Nguy√™n l√Ω Ng≈© h√†nh, B√°t qu√°i

Tr·∫£ l·ªùi C·ª∞C K·ª≤ NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-5 c√¢u), t·∫≠p trung v√†o th·ª±c t·∫ø, kh√¥ng l√Ω thuy·∫øt su√¥ng."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    def explain_element(self, element_type, element_name):
        """
        Explain element with context
        """
        # Update context
        self.update_context(
            last_action=f"Gi·∫£i th√≠ch {element_type}: {element_name}"
        )
        
        context = self.get_context_prompt()
        
        type_map = {
            'star': 'Tinh (Sao)',
            'door': 'M√¥n (C·ª≠a)',
            'deity': 'Th·∫ßn',
            'stem': 'Can (Thi√™n Can)'
        }
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

H√£y gi·∫£i th√≠ch C·ªêT L√ïI v·ªÅ {type_map.get(element_type, element_type)}: **{element_name}**

**Y√™u c·∫ßu (T·ªëi ƒëa 3-4 d√≤ng):**
1. B·∫£n ch·∫•t c·ªët l√µi (C√°t/Hung/Ng≈© h√†nh).
2. T√°c ƒë·ªông ch√≠nh ƒë·∫øn v·∫≠n m·ªánh/c√¥ng vi·ªác.
3. L·ªùi khuy√™n nhanh khi g·∫∑p y·∫øu t·ªë n√†y.

B·ªè qua ngu·ªìn g·ªëc, v√≠ d·ª• hay d·∫´n gi·∫£i d√†i d√≤ng. Tr·∫£ l·ªùi s·∫Øc b√©n, s√∫c t√≠ch."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
