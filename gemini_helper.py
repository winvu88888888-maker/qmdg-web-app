"""
Enhanced Gemini Helper with Context Awareness
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json

class GeminiQMDGHelper:
    """Helper class with context awareness for QMDG analysis"""
    
    def __init__(self, api_key):
        """Initialize Gemini with API key"""
        self.api_key = api_key
        genai.configure(api_key=api_key)
        
        # Context tracking - Initialize BEFORE model selection
        self.current_context = {
            'topic': None,
            'palace': None,
            'chart_data': None,
            'last_action': None,
            'dung_than': []
        }
        
        # Adaptive model selection
        self.model = self._get_best_model()

        # n8n endpoint (optional)
        self.n8n_url = None
    
    def set_n8n_url(self, url):
        """Set n8n webhook URL for processing"""
        self.n8n_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Prioritize 1.5 Pro because "gemini t·ªët nh·∫•t"
        models_to_try = [
            'gemini-2.0-flash-exp', # Try latest 2.0 flash
            'gemini-1.5-pro-latest', 
            'gemini-1.5-pro',
            'gemini-1.5-flash-latest', 
            'gemini-1.5-flash',
            'gemini-pro', 
            'gemini-1.0-pro'
        ]
        
        last_error = "Unknown error"
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Quick test with low tokens
                model.generate_content("ping", generation_config={"max_output_tokens": 1})
                return model
            except Exception as e:
                last_error = str(e)
                continue
        
        # Fallback to list models if configured ones fail
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    name = m.name.split('/')[-1]
                    try:
                        model = genai.GenerativeModel(name)
                        model.generate_content("ping", generation_config={"max_output_tokens": 1})
                        return model
                    except: continue
        except Exception: pass
        
        # Ultimate fallback but store error info
        self.last_startup_error = last_error
        return genai.GenerativeModel('gemini-1.5-flash') # Default to flash as it's more widely available

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?", generation_config={"max_output_tokens": 20})
            if response.text:
                return True, "K·∫øt n·ªëi th√†nh c√¥ng!"
            return False, "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
        except Exception as e:
            error_msg = str(e)
            if "API_KEY_INVALID" in error_msg:
                return False, "API Key kh√¥ng ch√≠nh x√°c ho·∫∑c ƒë√£ h·∫øt h·∫°n."
            elif "quota" in error_msg.lower():
                return False, "ƒê√£ h·∫øt h·∫°n m·ª©c s·ª≠ d·ª•ng (Quota) cho Key n√†y."
            return False, f"L·ªói k·∫øt n·ªëi: {error_msg}"

    def _call_ai(self, prompt):
        """Call AI via n8n or direct Gemini API"""
        # Option 1: Use n8n if configured
        if self.n8n_url:
            try:
                payload = {
                    "prompt": prompt,
                    "api_key": self.api_key
                }
                headers = {"Content-Type": "application/json"}
                response = requests.post(self.n8n_url, json=payload, headers=headers, timeout=60)
                if response.status_code == 200:
                    text = response.json().get('text', '')
                    if text: return text
                    # If empty text, fallback might be needed or return empty
                else:
                    print(f"n8n Error: {response.text}")
            except Exception as e:
                print(f"n8n Exception: {e}")
                # Fallback to local
        
        # Option 2: Direct Gemini API
        try:
            response = self.model.generate_content(prompt)
            if not response.text:
                return "‚ö†Ô∏è AI tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ªëng. Th·ª≠ l·∫°i sau ho·∫∑c ki·ªÉm tra API Key."
            return response.text
        except Exception as e:
            error_msg = str(e)
            if "finish_reason: SAFETY" in error_msg:
                return "üõ°Ô∏è N·ªôi dung b·ªã AI ch·∫∑n do vi ph·∫°m quy t·∫Øc an to√†n. Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c."
            raise e # Let the helper handle more complex errors if needed
    
    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        knowledge = """
**QUY T·∫ÆC LU·∫¨N GI·∫¢I CHUY√äN S√ÇU:**
1. **Nguy√™n l√Ω Sinh Kh·∫Øc Cung:** 
   - Th·ªßy (1) -> M·ªôc (3,4) -> H·ªèa (9) -> Th·ªï (2,8,5) -> Kim (6,7) -> Th·ªßy (1).
   - Kh·∫Øc: Th·ªßy kh·∫Øc H·ªèa, H·ªèa kh·∫Øc Kim, Kim kh·∫Øc M·ªôc, M·ªôc kh·∫Øc Th·ªï, Th·ªï kh·∫Øc Th·ªßy.
2. **D·ª•ng Th·∫ßn (Object):** L√† y·∫øu t·ªë ƒë·∫°i di·ªán cho s·ª± vi·ªác c·∫ßn xem.
3. **B·∫£n Th√¢n (Subject):** ƒê·∫°i di·ªán b·ªüi Can Ng√†y (Thi√™n b√†n) ho·∫∑c cung c·ªßa ng∆∞·ªùi h·ªèi.
4. **Ph√¢n t√≠ch n·ªôi cung:** 
   - Sao (Thi√™n th·ªùi), M√¥n (ƒê·ªãa l·ª£i - Nh√¢n h√≤a), Th·∫ßn (Th·∫ßn tr·ª£), Kh√¥ng Vong (Tr·∫°ng th√°i r·ªóng, ch∆∞a t·ªõi l√∫c ho·∫∑c th·∫•t b·∫°i).
5. **K·∫æT LU·∫¨N:** D·ª±a tr√™n vi·ªác Cung D·ª•ng Th·∫ßn Sinh cho hay Kh·∫Øc Cung B·∫£n Th·∫ßn (ho·∫∑c ng∆∞·ª£c l·∫°i).
"""
        return knowledge

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        
        # Add system-wide knowledge
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if self.current_context.get('palace'):
            palace = self.current_context['palace']
            context_parts.append(f"**ƒêang xem cung:** Cung {palace.get('num', 'N/A')} - {palace.get('qua', 'N/A')}")
            context_parts.append(f"  - Sao: {palace.get('star', 'N/A')}")
            context_parts.append(f"  - M√¥n: {palace.get('door', 'N/A')}")
            context_parts.append(f"  - Th·∫ßn: {palace.get('deity', 'N/A')}")
        
        if self.current_context.get('dung_than'):
            context_parts.append(f"**D·ª•ng Th·∫ßn:** {', '.join(self.current_context['dung_than'])}")
        
        if self.current_context.get('last_action'):
            context_parts.append(f"**H√†nh ƒë·ªông tr∆∞·ªõc:** {self.current_context['last_action']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH V√Ä KI·∫æN TH·ª®C HI·ªÜN T·∫†I:**"] + context_parts) + "\n\n"
        return ""
    
    def analyze_palace(self, palace_data, topic):
        """
        Analyze a specific palace with AI - WITH CONTEXT
        """
        # Update context
        self.update_context(
            topic=topic,
            palace=palace_data,
            last_action=f"Ph√¢n t√≠ch Cung {palace_data.get('num')}"
        )
        
        context = self.get_context_prompt()
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p h√†ng ƒë·∫ßu.

H√£y ph√¢n t√≠ch Cung {palace_data.get('num', 'N/A')} n√†y m·ªôt c√°ch CHUY√äN S√ÇU theo ƒë√∫ng ch·ªß ƒë·ªÅ: **{topic}**.

**Th√¥ng tin cung:**
- Sao: {palace_data.get('star', 'N/A')}, M√¥n: {palace_data.get('door', 'N/A')}, Th·∫ßn: {palace_data.get('deity', 'N/A')}
- Can: {palace_data.get('can_thien', 'N/A')}/{palace_data.get('can_dia', 'N/A')}

**Y√™u c·∫ßu ph√¢n t√≠ch:**
1. **M·ªëi li√™n h·ªá v·ªõi ch·ªß ƒë·ªÅ**: Cung n√†y ƒë√≥ng vai tr√≤ g√¨ trong c√¢u chuy·ªán "{topic}"? N√≥ l√† cung D·ª•ng Th·∫ßn, cung B·∫£n Th√¢n hay cung g√¢y ·∫£nh h∆∞·ªüng?
2. **Lu·∫≠n gi·∫£i t·ªï h·ª£p**: S·ª± k·∫øt h·ª£p gi·ªØa Sao {palace_data.get('star', 'N/A')} v√† M√¥n {palace_data.get('door', 'N/A')} t·∫°i ƒë√¢y c√≥ gi√∫p √≠ch hay c·∫£n tr·ªü cho m·ª•c ti√™u c·ªßa b·∫°n?
3. **Th·∫ø tr·∫≠n Th·∫ßn & Can**: Th·∫ßn {palace_data.get('deity', 'N/A')} v√† c·∫∑p Can {palace_data.get('can_thien', 'N/A')}/{palace_data.get('can_dia', 'N/A')} ƒëang t·∫°o ra c∆° h·ªôi hay c·∫°m b·∫´y n√†o?
4. **L·ªùi khuy√™n h√†nh ƒë·ªông**: N·∫øu b·∫°n quan t√¢m ƒë·∫øn "{topic}", b·∫°n c·∫ßn l∆∞u √Ω ƒëi·ªÅu g√¨ ƒë·∫∑c bi·ªát t·∫°i cung n√†y?

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, s√∫c s·∫Øc v√† mang t√≠nh ch·∫•t t∆∞ v·∫•n chuy√™n m√¥n."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i."
    
    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None):
        """
        Focused Relational Analysis: Self (Subject) vs Topic (Object)
        """
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            dung_than=dung_than_info or [],
            last_action="Lu·∫≠n gi·∫£i tr·ªçng t√¢m D·ª•ng Th·∫ßn"
        )
        
        # 1. SCAN FOR CORE ACTORS
        can_ngay = chart_data.get('can_ngay', 'N/A')
        can_gio = chart_data.get('can_gio', 'N/A')
        truc_phu = chart_data.get('truc_phu_ten', 'N/A')
        truc_su = chart_data.get('truc_su_ten', 'N/A')
        
        self_palace = "?"
        dung_than_details = []
        process_palace = "?"
        
        for i in range(1, 10):
            # Locate Self
            if chart_data.get('can_thien_ban', {}).get(i) == can_ngay:
                self_palace = str(i)
            # Locate Process/Outcome
            if chart_data.get('can_thien_ban', {}).get(i) == can_gio:
                process_palace = str(i)
            # Locate D·ª•ng Th·∫ßn
            if dung_than_info:
                for dt in dung_than_info:
                    if (chart_data.get('thien_ban', {}).get(i) == dt or 
                        chart_data.get('nhan_ban', {}).get(i) == dt or 
                        chart_data.get('than_ban', {}).get(i) == dt or 
                        chart_data.get('can_thien_ban', {}).get(i) == dt or
                        (dt.endswith(" M√¥n") and chart_data.get('nhan_ban', {}).get(i) in dt)):
                        # Get details of D·ª•ng Th·∫ßn Palace
                        detail = {
                            'dt': dt,
                            'palace': i,
                            'star': chart_data.get('thien_ban', {}).get(i),
                            'door': chart_data.get('nhan_ban', {}).get(i),
                            'deity': chart_data.get('than_ban', {}).get(i),
                            'void': i in chart_data.get('khong_vong', [])
                        }
                        dung_than_details.append(detail)
        
        # 2. CONTEXTUAL PROMPT
        prompt = f"""{self.get_context_prompt()}B·∫°n l√† b·∫≠c th·∫ßy K·ª≥ M√¥n ƒê·ªôn Gi√°p. H√£y th·ª±c hi·ªán lu·∫≠n gi·∫£i LU·∫¨N GI·∫¢I TR·ªåNG T√ÇM cho ch·ªß ƒë·ªÅ: **{topic}**.

**QUY T·∫ÆC C·ªêT L√ïI:**
- KH√îNG li·ªát k√™ t·∫•t c·∫£ 9 cung. 
- CH·ªà t·∫≠p trung v√†o m·ªëi quan h·ªá gi·ªØa **B·∫£n Th√¢n ({can_ngay})** v√† **D·ª•ng Th·∫ßn ({topic})**.
- K·∫æT LU·∫¨N d·ª©t kho√°t d·ª±a tr√™n Sinh/Kh·∫Øc/Ch·∫ø/H√≥a.

**TH√îNG TIN KEY TRONG B√ÄN:**
1. **B·∫£n Th√¢n (Ng∆∞·ªùi h·ªèi):** Cung {self_palace} (Sao {chart_data.get('thien_ban', {}).get(int(self_palace) if self_palace.isdigit() else 1)}, M√¥n {chart_data.get('nhan_ban', {}).get(int(self_palace) if self_palace.isdigit() else 1)}).
2. **D·ª•ng Th·∫ßn ({topic}):** {', '.join([f"{d['dt']} t·∫°i Cung {d['palace']} (Sao {d['star']}, M√¥n {d['door']}, Th·∫ßn {d['deity']}{', KH√îNG VONG' if d['void'] else ''})" for d in dung_than_details])}.
3. **Di·ªÖn bi·∫øn (Can Gi·ªù):** Cung {process_palace}.
4. **C∆° c·∫•u l√£nh ƒë·∫°o:** Tr·ª±c Ph√π l√† {truc_phu}, Tr·ª±c S·ª≠ l√† {truc_su}.

**Y√äU C·∫¶U N·ªòI DUNG:**

- **PH·∫¶N 1: T∆Ø∆†NG T√ÅC CH·ª¶ - KH√ÅCH (Quan tr·ªçng nh·∫•t):** Cung D·ª•ng Th·∫ßn ƒëang Sinh hay Kh·∫Øc Cung B·∫£n Th√¢n? ƒêi·ªÅu n√†y c√≥ nghƒ©a l√† s·ª± vi·ªác thu·∫≠n l·ª£i hay ƒëang g√¢y √°p l·ª±c cho b·∫°n? D·ª•ng Th·∫ßn b·ªã Kh√¥ng Vong hay D·ªãch M√£ ·∫£nh h∆∞·ªüng th·ª±c t·∫ø th·∫ø n√†o?
- **PH·∫¶N 2: DI·ªÑN BI·∫æN & TH·ª∞C THI:** Tr·ª±c Ph√π (xu th·∫ø l·ªõn) v√† Tr·ª±c S·ª≠ (c√°ch h√†nh ƒë·ªông) c√≥ ·ªßng h·ªô m·ª•c ti√™u "{topic}" kh√¥ng? 
- **PH·∫¶N 3: K·∫æT LU·∫¨N & CHI·∫æN L∆Ø·ª¢C:** 
    - K·∫øt qu·∫£ cu·ªëi c√πng l√† g√¨? 
    - N·∫øu X·∫§U (Kh·∫Øc): C·∫ßn d√πng y·∫øu t·ªë g√¨ ƒë·ªÉ h√≥a gi·∫£i (Th√¥ng quan)? 
    - N·∫øu T·ªêT (Sinh): C·∫ßn l√†m g√¨ ƒë·ªÉ th√∫c ƒë·∫©y nhanh h∆°n?
- **PH·∫¶N 4: ·ª®NG K·ª≤:** D·ª± ƒëo√°n th·ªùi ƒëi·ªÉm m·∫•u ch·ªët.

H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, s√∫c t√≠ch, s·∫Øc s·∫£o, t·∫≠p trung 100% v√†o ch·ªß ƒë·ªÅ {topic}."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS
        """
        # Use stored context if not provided
        if chart_data is None:
            chart_data = self.current_context.get('chart_data')
        if topic is None:
            topic = self.current_context.get('topic', 'Chung')
        
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            last_action=f"H·ªèi: {question[:50]}..."
        )
        
        context = self.get_context_prompt()
        
        # Build chart context if available
        chart_context = ""
        if chart_data:
            palace_summary = []
            for i in range(1, 10):
                palace_summary.append(
                    f"Cung {i}: {chart_data.get('thien_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('nhan_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('than_ban', {}).get(i, 'N/A')}"
                )
            chart_context = "\n**B√†n K·ª≥ M√¥n hi·ªán t·∫°i:**\n" + "\n".join(palace_summary)
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

**B·ªëi c·∫£nh:**
- Ch·ªß ƒë·ªÅ: {topic}
{chart_context}

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n:
1. Ng·ªØ c·∫£nh hi·ªán t·∫°i (ch·ªß ƒë·ªÅ, cung ƒëang xem, h√†nh ƒë·ªông tr∆∞·ªõc)
2. Th√¥ng tin t·ª´ b√†n K·ª≥ M√¥n (n·∫øu c√≥)
3. Ki·∫øn th·ª©c v·ªÅ d·ªãch h·ªçc
4. Nguy√™n l√Ω Ng≈© h√†nh, B√°t qu√°i

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, c·ª• th·ªÉ v√† th·ª±c t·∫ø b·∫±ng ti·∫øng Vi·ªát."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    def explain_element(self, element_type, element_name):
        """
        Explain element with context
        """
        # Update context
        self.update_context(
            last_action=f"Gi·∫£i th√≠ch {element_type}: {element_name}"
        )
        
        context = self.get_context_prompt()
        
        type_map = {
            'star': 'Tinh (Sao)',
            'door': 'M√¥n (C·ª≠a)',
            'deity': 'Th·∫ßn',
            'stem': 'Can (Thi√™n Can)'
        }
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

H√£y gi·∫£i th√≠ch chi ti·∫øt v·ªÅ {type_map.get(element_type, element_type)}: **{element_name}**

Bao g·ªìm:
1. Ngu·ªìn g·ªëc v√† √Ω nghƒ©a
2. Thu·ªôc t√≠nh (Ng≈© h√†nh, √¢m d∆∞∆°ng, v.v.)
3. T√≠nh ch·∫•t (c√°t/hung, ƒë·∫∑c ƒëi·ªÉm)
4. ·ª®ng d·ª•ng trong lu·∫≠n ƒëo√°n
5. V√≠ d·ª• c·ª• th·ªÉ

Gi·∫£i th√≠ch d·ªÖ hi·ªÉu, b·∫±ng ti·∫øng Vi·ªát."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
