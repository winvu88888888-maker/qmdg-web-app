"""
Enhanced Gemini Helper with Context Awareness
Gemini s·∫Ω t·ª± ƒë·ªông bi·∫øt ng·ªØ c·∫£nh: cung n√†o, ch·ªß ƒë·ªÅ g√¨, ƒëang xem ph·∫ßn n√†o
"""

import google.generativeai as genai
import os
import requests
import json

CUNG_NGU_HANH = {
    1: "Th·ªßy",
    2: "Th·ªï",
    3: "M·ªôc",
    4: "M·ªôc",
    5: "Th·ªï",
    6: "Kim",
    7: "Kim",
    8: "Th·ªï",
    9: "H·ªèa"
}

class GeminiQMDGHelper:
    """Helper class with context awareness for QMDG analysis"""
    
    def __init__(self, api_key):
        """Initialize Gemini with API key"""
        self.api_key = api_key
        genai.configure(api_key=api_key)
        
        # Context tracking - Initialize BEFORE model selection
        self.current_context = {
            'topic': None,
            'palace': None,
            'chart_data': None,
            'last_action': None,
            'dung_than': []
        }
        
        # Adaptive model selection
        self.model = self._get_best_model()

        # n8n endpoint (optional)
        self.n8n_url = None
    
    def set_n8n_url(self, url):
        """Set n8n webhook URL for processing"""
        self.n8n_url = url

    def _get_best_model(self):
        """Find the best available model for the current API key"""
        # Prioritize 1.5 Pro because "gemini t·ªët nh·∫•t"
        models_to_try = [
            'gemini-2.0-flash-exp', # Try latest 2.0 flash
            'gemini-1.5-pro-latest', 
            'gemini-1.5-pro',
            'gemini-1.5-flash-latest', 
            'gemini-1.5-flash',
            'gemini-pro', 
            'gemini-1.0-pro'
        ]
        
        last_error = "Unknown error"
        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)
                # Quick test with low tokens
                model.generate_content("ping", generation_config={"max_output_tokens": 1})
                return model
            except Exception as e:
                last_error = str(e)
                continue
        
        # Fallback to list models if configured ones fail
        try:
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    name = m.name.split('/')[-1]
                    try:
                        model = genai.GenerativeModel(name)
                        model.generate_content("ping", generation_config={"max_output_tokens": 1})
                        return model
                    except: continue
        except Exception: pass
        
        # Ultimate fallback but store error info
        self.last_startup_error = last_error
        return genai.GenerativeModel('gemini-1.5-flash') # Default to flash as it's more widely available

    def test_connection(self):
        """Quickly test if the API key and model are working"""
        try:
            response = self.model.generate_content("Xin ch√†o, b·∫°n c√≥ kh·ªèe kh√¥ng?", generation_config={"max_output_tokens": 20})
            if response.text:
                return True, "K·∫øt n·ªëi th√†nh c√¥ng!"
            return False, "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
        except Exception as e:
            error_msg = str(e)
            if "API_KEY_INVALID" in error_msg:
                return False, "API Key kh√¥ng ch√≠nh x√°c ho·∫∑c ƒë√£ h·∫øt h·∫°n."
            elif "quota" in error_msg.lower():
                return False, "ƒê√£ h·∫øt h·∫°n m·ª©c s·ª≠ d·ª•ng (Quota) cho Key n√†y."
            return False, f"L·ªói k·∫øt n·ªëi: {error_msg}"

    def _call_ai(self, prompt):
        """Call AI via n8n or direct Gemini API"""
        # Option 1: Use n8n if configured
        if self.n8n_url:
            try:
                payload = {
                    "prompt": prompt,
                    "api_key": self.api_key
                }
                headers = {"Content-Type": "application/json"}
                response = requests.post(self.n8n_url, json=payload, headers=headers, timeout=60)
                if response.status_code == 200:
                    text = response.json().get('text', '')
                    if text: return text
                    # If empty text, fallback might be needed or return empty
                else:
                    print(f"n8n Error: {response.text}")
            except Exception as e:
                print(f"n8n Exception: {e}")
                # Fallback to local
        
        # Option 2: Direct Gemini API
        try:
            response = self.model.generate_content(prompt)
            if not response.text:
                return "‚ö†Ô∏è AI tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ªëng. Th·ª≠ l·∫°i sau ho·∫∑c ki·ªÉm tra API Key."
            return response.text
        except Exception as e:
            error_msg = str(e)
            if "finish_reason: SAFETY" in error_msg:
                return "üõ°Ô∏è N·ªôi dung b·ªã AI ch·∫∑n do vi ph·∫°m quy t·∫Øc an to√†n. Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c."
            raise e # Let the helper handle more complex errors if needed
    
    def update_context(self, **kwargs):
        """Update current context"""
        self.current_context.update(kwargs)
    
    def get_system_knowledge(self):
        """Returns string representation of key system rules for AI context"""
        knowledge = """
**QUY T·∫ÆC LU·∫¨N GI·∫¢I CHUY√äN S√ÇU:**
1. **Nguy√™n l√Ω Sinh Kh·∫Øc Cung:** 
   - Th·ªßy (1) -> M·ªôc (3,4) -> H·ªèa (9) -> Th·ªï (2,8,5) -> Kim (6,7) -> Th·ªßy (1).
   - Kh·∫Øc: Th·ªßy kh·∫Øc H·ªèa, H·ªèa kh·∫Øc Kim, Kim kh·∫Øc M·ªôc, M·ªôc kh·∫Øc Th·ªï, Th·ªï kh·∫Øc Th·ªßy.
2. **D·ª•ng Th·∫ßn (Object):** L√† y·∫øu t·ªë ƒë·∫°i di·ªán cho s·ª± vi·ªác c·∫ßn xem.
3. **B·∫£n Th√¢n (Subject):** ƒê·∫°i di·ªán b·ªüi Can Ng√†y (Thi√™n b√†n) ho·∫∑c cung c·ªßa ng∆∞·ªùi h·ªèi.
4. **Ph√¢n t√≠ch n·ªôi cung:** 
   - Sao (Thi√™n th·ªùi), M√¥n (ƒê·ªãa l·ª£i - Nh√¢n h√≤a), Th·∫ßn (Th·∫ßn tr·ª£), Kh√¥ng Vong (Tr·∫°ng th√°i r·ªóng, ch∆∞a t·ªõi l√∫c ho·∫∑c th·∫•t b·∫°i).
5. **K·∫æT LU·∫¨N:** D·ª±a tr√™n vi·ªác Cung D·ª•ng Th·∫ßn Sinh cho hay Kh·∫Øc Cung B·∫£n Th·∫ßn (ho·∫∑c ng∆∞·ª£c l·∫°i).
"""
        return knowledge

    def get_context_prompt(self):
        """Build context prompt from current state"""
        context_parts = []
        
        # Add system-wide knowledge
        context_parts.append(self.get_system_knowledge())
        
        if self.current_context.get('topic'):
            context_parts.append(f"**Ch·ªß ƒë·ªÅ hi·ªán t·∫°i:** {self.current_context['topic']}")
        
        if self.current_context.get('palace'):
            palace = self.current_context['palace']
            context_parts.append(f"**ƒêang xem cung:** Cung {palace.get('num', 'N/A')} - {palace.get('qua', 'N/A')}")
            context_parts.append(f"  - Sao: {palace.get('star', 'N/A')}")
            context_parts.append(f"  - M√¥n: {palace.get('door', 'N/A')}")
            context_parts.append(f"  - Th·∫ßn: {palace.get('deity', 'N/A')}")
        
        if self.current_context.get('dung_than'):
            context_parts.append(f"**D·ª•ng Th·∫ßn:** {', '.join(self.current_context['dung_than'])}")
        
        if self.current_context.get('last_action'):
            context_parts.append(f"**H√†nh ƒë·ªông tr∆∞·ªõc:** {self.current_context['last_action']}")
        
        if context_parts:
            return "\n".join(["**NG·ªÆ C·∫¢NH V√Ä KI·∫æN TH·ª®C HI·ªÜN T·∫†I:**"] + context_parts) + "\n\n"
        return ""
    
    def analyze_palace(self, palace_data, topic):
        """
        Analyze a specific palace with AI - FOCUS ON ESSENTIALS
        """
        # Update context
        self.update_context(
            topic=topic,
            palace=palace_data,
            last_action=f"Ph√¢n t√≠ch Cung {palace_data.get('num')}"
        )
        
        context = self.get_context_prompt()
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p h√†ng ƒë·∫ßu. H√£y ph√¢n t√≠ch Cung {palace_data.get('num', 'N/A')} cho ch·ªß ƒë·ªÅ: **{topic}**.

**NGUY√äN T·∫ÆC: NG·∫ÆN G·ªåN - ƒê·ª¶ √ù - KH√îNG LAN MAN.**

**Y√™u c·∫ßu:**
1. **Gi√° tr·ªã c·ªßa cung**: Cung n√†y l√† Thu·∫≠n hay Ngh·ªãch cho vi·ªác "{topic}"?
2. **ƒêi·ªÉm nh·∫•n ch√≠nh**: T·ªï h·ª£p Sao/M√¥n/Th·∫ßn/Can t·∫°i ƒë√¢y b√°o hi·ªáu ƒëi·ªÅu g√¨ c·ªët l√µi nh·∫•t?
3. **Chi·∫øn thu·∫≠t h√†nh ƒë·ªông**: L√†m g√¨ ngay t·∫°i cung n√†y ƒë·ªÉ ƒë·∫°t m·ª•c ti√™u?

Tr·∫£ l·ªùi s√∫c t√≠ch, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng ch√†o h·ªèi, kh√¥ng d·∫´n nh·∫≠p."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}\n\nVui l√≤ng ki·ªÉm tra API key ho·∫∑c th·ª≠ l·∫°i."
    
    def comprehensive_analysis(self, chart_data, topic, dung_than_info=None, topic_hints="", subj_stem=None, obj_stem=None):
        """
        Laser-Focused Analysis: Interaction between specific D·ª•ng Th·∫ßn palaces.
        With Dynamic Subject (Ch·ªß) and Object (Kh√°ch) identification.
        """
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            dung_than=dung_than_info or [],
            last_action="Lu·∫≠n gi·∫£i ƒëa t·∫ßng Laser-Focused (Dynamic Actors)"
        )
        
        can_ngay = chart_data.get('can_ngay', 'N/A')
        can_gio = chart_data.get('can_gio', 'N/A')
        truc_phu = chart_data.get('truc_phu_ten', 'N/A')
        truc_su = chart_data.get('truc_su_ten', 'N/A')
        khong_vong = chart_data.get('khong_vong', [])
        
        # Determine actual actors for this session
        final_subj_stem = subj_stem if subj_stem else can_ngay
        final_obj_stem = obj_stem if obj_stem else can_gio
        
        # 1. GROUP DATA BY PALACE
        palaces_of_interest = {} # {palace_num: {info}}
        
        def add_to_poi(p_num, label):
            if p_num not in palaces_of_interest:
                palaces_of_interest[p_num] = {
                    'labels': [],
                    'star': chart_data.get('thien_ban', {}).get(p_num, 'N/A'),
                    'door': chart_data.get('nhan_ban', {}).get(p_num, 'N/A'),
                    'deity': chart_data.get('than_ban', {}).get(p_num, 'N/A'),
                    'can_thien': chart_data.get('can_thien_ban', {}).get(p_num, 'N/A'),
                    'can_dia': chart_data.get('dia_can', {}).get(p_num, 'N/A'),
                    'hanh': CUNG_NGU_HANH.get(p_num, 'N/A'),
                    'void': p_num in khong_vong
                }
            if label not in palaces_of_interest[p_num]['labels']:
                palaces_of_interest[p_num]['labels'].append(label)

        # Scan all palaces for actors and Useful Gods
        for i in range(1, 10):
            # 1. Check Subject (Self/As selected)
            if chart_data.get('can_thien_ban', {}).get(i) == final_subj_stem:
                add_to_poi(i, f"B·∫£n Th√¢n/Ch·ªß Th·ªÉ ({final_subj_stem})")
            
            # 2. Check Object (Target/As selected)
            if chart_data.get('can_thien_ban', {}).get(i) == final_obj_stem:
                add_to_poi(i, f"ƒê·ªëi T∆∞·ª£ng/Kh√°ch ({final_obj_stem})")
            
            # 3. Check other D·ª•ng Th·∫ßn
            if dung_than_info:
                for dt in dung_than_info:
                    door_val = chart_data.get('nhan_ban', {}).get(i)
                    if (chart_data.get('thien_ban', {}).get(i) == dt or 
                        door_val == dt or 
                        chart_data.get('than_ban', {}).get(i) == dt or 
                        chart_data.get('can_thien_ban', {}).get(i) == dt or
                        (dt.endswith(" M√¥n") and door_val and door_val in dt)):
                        add_to_poi(i, dt)
        
        # 2. CONTEXTUAL PROMPT
        poi_desc = []
        for p_num, info in palaces_of_interest.items():
            labels_str = ", ".join(info['labels'])
            void_str = " [KH√îNG VONG]" if info['void'] else ""
            desc = (f"Cung {p_num} ({info['hanh']}): Ch·ª©a {labels_str}. "
                    f"Tr·∫≠n th·∫ø: {info['star']} - {info['door']} - {info['deity']}. "
                    f"C·∫∑p Can: {info['can_thien']}/{info['can_dia']}{void_str}")
            poi_desc.append(desc)

        prompt = f"""{self.get_context_prompt()}B·∫°n l√† b·∫≠c th·∫ßy K·ª≥ M√¥n ƒê·ªôn Gi√°p cao c·∫•p. H√£y th·ª±c hi·ªán LU·∫¨N GI·∫¢I CHUY√äN S√ÇU TAM GI√ÅC cho ch·ªß ƒë·ªÅ: **{topic}**.

**NGUY√äN T·∫ÆC LU·∫¨N GI·∫¢I B·∫ÆT BU·ªòC:**
1. **Ph√¢n t√≠ch N·ªôi T·∫°i (Quan tr·ªçng)**: ƒê√°nh gi√° s·ª©c m·∫°nh n·ªôi t·∫°i c·ªßa **Ch·ªß Th·ªÉ ({final_subj_stem})** - ng∆∞·ªùi ch√∫ng ta ƒëang h·ªèi gi√∫p. H·ªç c√≥ ƒë·ªß l·ª±c, ƒë·ªß thu·∫≠n l·ª£i ƒë·ªÉ th·ª±c hi·ªán vi·ªác n√†y kh√¥ng?
2. **Lu·∫≠n gi·∫£i Tam Gi√°c (Triangular Logic)**: Ph√¢n t√≠ch s·ª± t∆∞∆°ng t√°c gi·ªØa 3 ƒë·ªânh: 
   - ƒê·ªânh 1: **Ch·ªß Th·ªÉ ({final_subj_stem})** - ƒê·∫°i di·ªán cho ng∆∞·ªùi th√¢n/ng∆∞·ªùi h·ªèi.
   - ƒê·ªânh 2: **ƒê·ªëi T∆∞·ª£ng ({final_obj_stem})** - ƒê·∫°i di·ªán cho ng∆∞·ªùi mua/ƒë·ªëi th·ªß/ng∆∞·ªùi l·∫°.
   - ƒê·ªânh 3: **D·ª•ng Th·∫ßn Topic** - ƒê·∫°i di·ªán cho c√°i nh√†/ti·ªÅn b·∫°c/k·∫øt qu·∫£ ({topic}).
3. **K·∫øt lu·∫≠n logic**: Li·ªáu Ch·ªß Th·ªÉ c√≥ th·∫Øng ƒë∆∞·ª£c ƒê·ªëi T∆∞·ª£ng ƒë·ªÉ chi·∫øm l·∫•y k·∫øt qu·∫£ kh√¥ng?

**D·ªÆ LI·ªÜU C√ÅC CUNG TR·ªåNG T√ÇM:**
{chr(10).join(poi_desc)}

**TH·∫æ TR·∫¨N T·ªîNG TH·ªÇ:**
- Xu th·∫ø (Tr·ª±c Ph√π): {truc_phu}
- Ch·∫•p h√†nh (Tr·ª±c S·ª≠): {truc_su}
- G·ª£i √Ω chuy√™n m√¥n: "{topic_hints}"

**N·ªòI DUNG B√ÅO C√ÅO (S√öC T√çCH - QUY·ªÄN L·ª∞C):**

- **PH·∫¶N 1: TR·∫†NG TH√ÅI C·ª¶A NG∆Ø·ªúI ƒê∆Ø·ª¢C XEM ({final_subj_stem})**: Ng∆∞·ªùi n√†y ƒëang m·∫°nh hay y·∫øu? Cung c·ªßa h·ªç c√≥ thu·∫≠n l·ª£i hay ƒëang g·∫∑p kh√≥ khƒÉn n·ªôi t·∫°i?
- **PH·∫¶N 2: T∆Ø∆†NG T√ÅC TAM GI√ÅC**: Ph√¢n t√≠ch quan h·ªá Sinh/Kh·∫Øc gi·ªØa Ng∆∞·ªùi n√†y vs ƒê·ªëi t∆∞·ª£ng vs D·ª•ng th·∫ßn ch·ªß ƒë·ªÅ.
- **PH·∫¶N 3: PH√ÅN QUY·∫æT CU·ªêI C√ôNG**: D·ª±a tr√™n b·ªëi c·∫£nh "{topic_hints}", ng∆∞·ªùi n√†y c√≥ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ƒë√≠ch kh√¥ng? T·∫°i sao?
- **PH·∫¶N 4: CHI·∫æN THU·∫¨T & ·ª®NG K·ª≤**: Ph·∫£i l√†m g√¨ ƒë·ªÉ gi√∫p ng∆∞·ªùi n√†y ƒë·∫°t m·ª•c ti√™u nhanh nh·∫•t? Khi n√†o?

Tr·∫£ l·ªùi b·∫±ng phong th√°i chuy√™n gia, t·∫≠p trung ho√†n to√†n v√†o vi·ªác gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ cho Ch·ªß Th·ªÉ."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói khi g·ªçi AI: {str(e)}"
    
    def answer_question(self, question, chart_data=None, topic=None):
        """
        Answer with FULL CONTEXT AWARENESS
        """
        # Use stored context if not provided
        if chart_data is None:
            chart_data = self.current_context.get('chart_data')
        if topic is None:
            topic = self.current_context.get('topic', 'Chung')
        
        # Update context
        self.update_context(
            topic=topic,
            chart_data=chart_data,
            last_action=f"H·ªèi: {question[:50]}..."
        )
        
        context = self.get_context_prompt()
        
        # Build chart context if available
        chart_context = ""
        if chart_data:
            palace_summary = []
            for i in range(1, 10):
                palace_summary.append(
                    f"Cung {i}: {chart_data.get('thien_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('nhan_ban', {}).get(i, 'N/A')} - "
                    f"{chart_data.get('than_ban', {}).get(i, 'N/A')}"
                )
            chart_context = "\n**B√†n K·ª≥ M√¥n hi·ªán t·∫°i:**\n" + "\n".join(palace_summary)
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

**B·ªëi c·∫£nh:**
- Ch·ªß ƒë·ªÅ: {topic}
{chart_context}

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n:
1. Ng·ªØ c·∫£nh hi·ªán t·∫°i (ch·ªß ƒë·ªÅ, cung ƒëang xem, h√†nh ƒë·ªông tr∆∞·ªõc)
2. Th√¥ng tin t·ª´ b√†n K·ª≥ M√¥n (n·∫øu c√≥)
3. Ki·∫øn th·ª©c v·ªÅ d·ªãch h·ªçc
4. Nguy√™n l√Ω Ng≈© h√†nh, B√°t qu√°i

Tr·∫£ l·ªùi C·ª∞C K·ª≤ NG·∫ÆN G·ªåN (t·ªëi ƒëa 3-5 c√¢u), t·∫≠p trung v√†o th·ª±c t·∫ø, kh√¥ng l√Ω thuy·∫øt su√¥ng."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
    
    def explain_element(self, element_type, element_name):
        """
        Explain element with context
        """
        # Update context
        self.update_context(
            last_action=f"Gi·∫£i th√≠ch {element_type}: {element_name}"
        )
        
        context = self.get_context_prompt()
        
        type_map = {
            'star': 'Tinh (Sao)',
            'door': 'M√¥n (C·ª≠a)',
            'deity': 'Th·∫ßn',
            'stem': 'Can (Thi√™n Can)'
        }
        
        prompt = f"""{context}B·∫°n l√† chuy√™n gia K·ª≥ M√¥n ƒê·ªôn Gi√°p.

H√£y gi·∫£i th√≠ch C·ªêT L√ïI v·ªÅ {type_map.get(element_type, element_type)}: **{element_name}**

**Y√™u c·∫ßu (T·ªëi ƒëa 3-4 d√≤ng):**
1. B·∫£n ch·∫•t c·ªët l√µi (C√°t/Hung/Ng≈© h√†nh).
2. T√°c ƒë·ªông ch√≠nh ƒë·∫øn v·∫≠n m·ªánh/c√¥ng vi·ªác.
3. L·ªùi khuy√™n nhanh khi g·∫∑p y·∫øu t·ªë n√†y.

B·ªè qua ngu·ªìn g·ªëc, v√≠ d·ª• hay d·∫´n gi·∫£i d√†i d√≤ng. Tr·∫£ l·ªùi s·∫Øc b√©n, s√∫c t√≠ch."""

        try:
            return self._call_ai(prompt)
        except Exception as e:
            return f"‚ùå L·ªói: {str(e)}"
